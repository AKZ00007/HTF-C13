{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e416a997",
   "metadata": {},
   "source": [
    "Model 1: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79811038",
   "metadata": {},
   "source": [
    "model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa192fcc",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d280d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # One-hot encode employee_skills\n",
    "    mlb_skills = MultiLabelBinarizer()\n",
    "    skills_encoded = mlb_skills.fit_transform(df['employee_skills'])\n",
    "    skills_df = pd.DataFrame(skills_encoded, columns=[f\"skill_{s}\" for s in mlb_skills.classes_])\n",
    "\n",
    "    # One-hot encode employee_availability (days 0–6)\n",
    "    mlb_avail = MultiLabelBinarizer()\n",
    "    avail_encoded = mlb_avail.fit_transform(df['employee_availability'])\n",
    "    avail_df = pd.DataFrame(avail_encoded, columns=[f\"avail_day_{d}\" for d in mlb_avail.classes_])\n",
    "\n",
    "    # One-hot encode task_required_skills\n",
    "    mlb_task_skills = MultiLabelBinarizer()\n",
    "    task_skills_encoded = mlb_task_skills.fit_transform(df['task_required_skills'])\n",
    "    task_skills_df = pd.DataFrame(task_skills_encoded, columns=[f\"task_skill_{s}\" for s in mlb_task_skills.classes_])\n",
    "\n",
    "    # Encode task_priority (low, medium, high)\n",
    "    df['task_priority'] = df['task_priority'].map({'low': 0, 'medium': 1, 'high': 2})\n",
    "\n",
    "    # Combine everything\n",
    "    final_df = pd.concat([\n",
    "        skills_df,\n",
    "        avail_df,\n",
    "        task_skills_df,\n",
    "        df[['task_priority', 'task_duration_days', 'task_start_day', 'rule_violated']]\n",
    "    ], axis=1)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58b4eb",
   "metadata": {},
   "source": [
    "UPSAMPLE CLASS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c81b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Split into majority and minority\n",
    "df_majority = df[df.assignment_valid == 0]\n",
    "df_minority = df[df.assignment_valid == 1]\n",
    "\n",
    "# Upsample the minority class\n",
    "df_minority_upsampled = resample(\n",
    "    df_minority,\n",
    "    replace=True,                      # Allow duplicates\n",
    "    n_samples=len(df_majority),        # Make it same size as majority\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine both\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Shuffle so they’re mixed well\n",
    "df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9511505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        95\n",
      "           1       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.47      0.50      0.49       100\n",
      "weighted avg       0.90      0.95      0.93       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ashwi\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ashwi\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "\n",
    "# Step 1: Load your JSON data\n",
    "df = pd.read_json(r'C:\\Users\\ashwi\\OneDrive\\Desktop\\Sycamore\\Hackathons\\HackToFuture sjec\\ML part\\scheduling_dataset.json')  # Replace with your path\n",
    "\n",
    "# Step 2: Preprocessing function\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Encode skills using MultiLabelBinarizer\n",
    "    mlb_skills = MultiLabelBinarizer()\n",
    "    employee_skill_encoded = mlb_skills.fit_transform(df['employee_skills'])\n",
    "    task_skill_encoded = mlb_skills.transform(df['task_required_skills'])\n",
    "\n",
    "    # Encode availability (one-hot)\n",
    "    mlb_avail = MultiLabelBinarizer()\n",
    "    availability_encoded = mlb_avail.fit_transform(df['employee_availability'])\n",
    "\n",
    "    # Encode priority using OneHotEncoder\n",
    "    enc = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    priority_encoded = enc.fit_transform(df[['task_priority']])\n",
    "\n",
    "    # Stack all features into one final array\n",
    "    features = pd.DataFrame(\n",
    "        data = pd.concat([\n",
    "            pd.DataFrame(employee_skill_encoded, columns=mlb_skills.classes_),\n",
    "            pd.DataFrame(task_skill_encoded, columns=[f\"task_{s}\" for s in mlb_skills.classes_]),\n",
    "            pd.DataFrame(availability_encoded, columns=[f\"avail_{i}\" for i in mlb_avail.classes_]),\n",
    "            pd.DataFrame(priority_encoded, columns=enc.get_feature_names_out(['task_priority'])),\n",
    "\n",
    "            df[['task_duration_days', 'task_start_day']]\n",
    "        ], axis=1)\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply preprocessing\n",
    "X = preprocess(df)\n",
    "y = df['assignment_valid']\n",
    "\n",
    "# Step 4: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13ec1b",
   "metadata": {},
   "source": [
    "Full working version with unsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16335a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        94\n",
      "           1       0.99      1.00      0.99        96\n",
      "\n",
      "    accuracy                           0.99       190\n",
      "   macro avg       0.99      0.99      0.99       190\n",
      "weighted avg       0.99      0.99      0.99       190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Step 1: Load data\n",
    "with open(r'C:\\Users\\ashwi\\OneDrive\\Desktop\\Sycamore\\Hackathons\\HackToFuture sjec\\ML part\\scheduling_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Step 2: Upsample to balance classes\n",
    "df_majority = df[df.assignment_valid == 0]\n",
    "df_minority = df[df.assignment_valid == 1]\n",
    "\n",
    "df_minority_upsampled = resample(\n",
    "    df_minority,\n",
    "    replace=True,\n",
    "    n_samples=len(df_majority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "def preprocess(df):\n",
    "    mlb_skills = MultiLabelBinarizer()\n",
    "    mlb_avail = MultiLabelBinarizer()\n",
    "\n",
    "    employee_skill_encoded = mlb_skills.fit_transform(df['employee_skills'])\n",
    "    task_skill_encoded = mlb_skills.transform(df['task_required_skills'])\n",
    "    availability_encoded = mlb_avail.fit_transform(df['employee_availability'])\n",
    "\n",
    "    enc = OneHotEncoder(sparse_output=False)  # sklearn >= 1.2 uses sparse_output\n",
    "    priority_encoded = enc.fit_transform(df[['task_priority']])\n",
    "\n",
    "    features = pd.DataFrame(\n",
    "        data = pd.concat([\n",
    "            pd.DataFrame(employee_skill_encoded, columns=mlb_skills.classes_),\n",
    "            pd.DataFrame(task_skill_encoded, columns=[f\"task_{s}\" for s in mlb_skills.classes_]),\n",
    "            pd.DataFrame(availability_encoded, columns=[f\"avail_{i}\" for i in mlb_avail.classes_]),\n",
    "            pd.DataFrame(priority_encoded, columns=enc.get_feature_names_out(['task_priority'])),\n",
    "            df[['task_duration_days', 'task_start_day']]\n",
    "        ], axis=1)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "X = preprocess(df)\n",
    "y = df['assignment_valid']\n",
    "\n",
    "# Step 4: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737990b",
   "metadata": {},
   "source": [
    "Model saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edfd47",
   "metadata": {},
   "source": [
    "#You're trying to save the encoders like mlb_skills, mlb_avail, and enc outside the preprocessing function, but they were defined inside it — so they’re not accessible from the outside right now.\n",
    "\n",
    "Let’s fix that neatly and make sure you can reuse the encoders when loading the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    global mlb_skills, mlb_avail, enc  # <- THIS makes them accessible outside\n",
    "\n",
    "    mlb_skills = MultiLabelBinarizer()\n",
    "    employee_skill_encoded = mlb_skills.fit_transform(df['employee_skills'])\n",
    "    task_skill_encoded = mlb_skills.transform(df['task_required_skills'])\n",
    "\n",
    "    mlb_avail = MultiLabelBinarizer()\n",
    "    availability_encoded = mlb_avail.fit_transform(df['employee_availability'])\n",
    "\n",
    "    enc = OneHotEncoder(sparse_output=False)\n",
    "    priority_encoded = enc.fit_transform(df[['task_priority']])\n",
    "\n",
    "    features = pd.DataFrame(\n",
    "        data = pd.concat([\n",
    "            pd.DataFrame(employee_skill_encoded, columns=mlb_skills.classes_),\n",
    "            pd.DataFrame(task_skill_encoded, columns=[f\"task_{s}\" for s in mlb_skills.classes_]),\n",
    "            pd.DataFrame(availability_encoded, columns=[f\"avail_{i}\" for i in mlb_avail.classes_]),\n",
    "            pd.DataFrame(priority_encoded, columns=enc.get_feature_names_out(['task_priority'])),\n",
    "            df[['task_duration_days', 'task_start_day']]\n",
    "        ], axis=1)\n",
    "    )\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b32566a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b068f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('smart_scheduler_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('encoders.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'mlb_skills': mlb_skills,\n",
    "        'mlb_avail': mlb_avail,\n",
    "        'priority_enc': enc\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af3febf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming df is your dataframe\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 1. Handle class imbalance with upsampling (your existing code)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df_majority \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[df\u001b[38;5;241m.\u001b[39massignment_valid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m df_minority \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39massignment_valid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Upsample the minority class\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Improved preprocessing with additional steps\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "# 1. Handle class imbalance with upsampling (your existing code)\n",
    "df_majority = df[df.assignment_valid == 0]\n",
    "df_minority = df[df.assignment_valid == 1]\n",
    "\n",
    "# Upsample the minority class\n",
    "df_minority_upsampled = resample(\n",
    "    df_minority,\n",
    "    replace=True,\n",
    "    n_samples=len(df_majority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine both\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Shuffle\n",
    "df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 2. Split features and target\n",
    "X = df_balanced.drop('assignment_valid', axis=1)\n",
    "y = df_balanced['assignment_valid']\n",
    "\n",
    "# 3. Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# 4. Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 5. Create train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Build pipeline with preprocessing and optimized Random Forest\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 7. Train the model\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluate\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 9. Get feature importance\n",
    "feature_names = (\n",
    "    numeric_features.tolist() + \n",
    "    list(rf_pipeline.named_steps['preprocessor']\n",
    "        .named_transformers_['cat']\n",
    "        .get_feature_names_out(categorical_features))\n",
    ")\n",
    "importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"\\nFeature ranking:\")\n",
    "for f in range(min(20, len(feature_names))):  # Top 20 features\n",
    "    print(f\"{f+1}. {feature_names[indices[f]]} ({importances[indices[f]]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
